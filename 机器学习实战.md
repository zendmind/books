第一章 机器学习基础
=

# 1.1 一些概念

机器学习是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。

## 监督学习
监督学习(supervised learning)：是机器学习中的一种方法，可以由训练数据中学习到或通过建立一个模式（函数 / learning model），并依此模式推测新的实例。训练数据是由输入数据（通常是向量）和预期输出所组成。函数的输出可以是一个连续的值（称为回归分析），或是预测一个分类标签（称作分类）。

通俗的说就是：监督学习算法训练含有很多特征的带标签（label 或 target）的数据集，来对新的数据集的标签做出预测。我们把需要训练的数据集称为训练集（trainset），需要预测的数据集称为测试集。

## 无监督学习
无监督学习(unsupervised learning)：已知数据不知道任何标签，按照一定的偏好，训练一个智能算法，将所有的数据映射到多个不同标签的过程。相对于有监督学习，无监督学习是一类比较困难的问题，所谓的按照一定的偏好，是比如特征空间距离最近，等人们认为属于一类的事物应具有的一些特点。举个例子，猪和鸵鸟混杂在一起，算法会测量高度，发现动物们主要集中在两个高度，一类动物身高一米左右，另一类动物身高半米左右，那么算法按照就近原则，75 厘米以上的就是高的那类也就是鸵鸟，矮的那类是第二类也就是猪，当然这里也会出现身材矮小的鸵鸟和身高爆表的猪会被错误的分类。

常见的非监督式学习是数据聚类。在人工神经网路中，生成对抗网络（GAN）、自组织映射（SOM）和适应性共振理论（ART）则是最常用的非监督式学习。

## 半监督学习
半监督学习是一种结合有监督学习和无监督学习的一种学习方式。它是近年来研究的热点，原因是在真正的模型建立的过程中，往往有类标的数据很少，而绝大多数的数据样本是没有确定类标的。这时候，我们无法直接应用有监督的学习方法进行模型的训练，因为有监督学习算法在有类标数据很少的情况下学习的效果往往很差。但是，我们也不能直接利用无监督学习的方式进行学习，因为这样，我们就没有充分的利用那些已给出的类标的有用信息。

## 强化学习
强化学习(reinforcement learning)：智能算法在没有人为指导的情况下，通过不断的试错来提升任务性能的过程。“试错”的意思是还是有一个衡量标准，用棋类游戏举例，我们并不知道棋手下一步棋是对是错，不知道哪步棋是制胜的关键，但是我们知道结果是输还是赢，如果算法这样走最后的结果是胜利，那么算法就学习记忆，如果按照那样走最后输了，那么算法就学习以后不这样走。

## 数据集
“数据集”（Data Set）：就是数据的集合的意思。其中，每一条单独的数据被称为“样本”（Sample）。若没有进行特殊说明，本书都会假设数据集中样本之间在各种意义下相互独立。事实上，除了某些特殊的模型（如隐马尔可夫模型和条件随机场），该假设在大多数场景下都是相当合理的。  
对于每个样本，它通常具有一些“属性”（Attribute）或说“特征”（Feature），特征所具体取的值就被称为“特征值”（Feature Value）。  
特征和样本所张成的空间被称为“特征空间”（Feature Space）和“样本空间”（Sample Space），可以把它们简单地理解为特征和样本“可能存在的空间”。  
相对应的，我们有“标签空间”（Label Space），它描述了模型的输出“可能存在的空间”；当模型是分类器时、我们通常会称之为“类别空间”。

其中、数据集可以分为以下三类：  
训练集（Training Set）；顾名思义、它是总的数据集中用来训练我们模型的部分。虽说将所有数据集都拿来当做训练集也无不可，不过为了提高及合理评估模型的泛化能力、我们通常只会取数据集中的一部分来当训练集。  
测试集（Test Set）；顾名思义、它是用来测试、评估模型泛化能力的部分。测试集不会用在模型的训练部分；换句话说，测试集相对于模型而言是“未知”的、所以拿它来评估模型的泛化能力是相当合理的。  
交叉验证集（Cross-Validation Set，简称 CV Set）；这是比较特殊的一部分数据，它是用来调整模型具体参数的。  

# 1.2 常见算法

## 回归算法(Regression Algorithms)
![git-pro-1](http://git.wangxutech.com/web/frontend/notes/books/raw/master/images/machine-learning/1.jpeg)  
回归算法是一种通过最小化预测值与实际结果值之间的差距，而得到输入特征之间的最佳组合方式的一类算法。对于连续值预测有线性回归等，而对于离散值/类别预测，我们也可以把逻辑回归等也视作回归算法的一种，常见的回归算法如下：

- Ordinary Least Squares Regression (OLSR)
- Linear Regression
- Logistic Regression
- Stepwise Regression
- Locally Estimated Scatterplot Smoothing (LOESS)
- Multivariate Adaptive Regression Splines (MARS)

## 基于实例的算法(Instance-based Algorithms)
![git-pro-1](http://git.wangxutech.com/web/frontend/notes/books/raw/master/images/machine-learning/2.jpg)  

这里所谓的基于实例的算法，我指的是我们最后建成的模型，对原始数据样本实例依旧有很强的依赖性。这类算法在做预测决策时，一般都是使用某类相似度准则，去比对待预测的样本和原始样本的相近度，再给出相应的预测结果。常见的基于实例的算法有：

- k-Nearest Neighbour (kNN)
- Learning Vector Quantization (LVQ)
- Self-Organizing Map (SOM)
- Locally Weighted Learning (LWL)

## 决策树类算法(Decision Tree Algorithms)
![git-pro-1](http://git.wangxutech.com/web/frontend/notes/books/raw/master/images/machine-learning/3.jpg)  

决策树类算法，会基于原始数据特征，构建一颗包含很多决策路径的树。预测阶段选择路径进行决策。常见的决策树算法包括：

- Classification and Regression Tree (CART)
- Iterative Dichotomiser 3 (ID3)
- C4.5 and C5.0 (different versions of a powerful approach)
- Chi-squared Automatic Interaction Detection (CHAID)
- M5
- Conditional Decision Trees

## 贝叶斯类算法(Bayesian Algorithms)
![git-pro-1](http://git.wangxutech.com/web/frontend/notes/books/raw/master/images/machine-learning/4.jpg)  

这里说的贝叶斯类算法，指的是在分类和回归问题中，隐含使用了贝叶斯原理的算法。包括：

Naive Bayes
Gaussian Naive Bayes
Multinomial Naive Bayes
Averaged One-Dependence Estimators (AODE)
Bayesian Belief Network (BBN)
Bayesian Network (BN)

## 聚类算法(Clustering Algorithms)
![git-pro-1](http://git.wangxutech.com/web/frontend/notes/books/raw/master/images/machine-learning/5.jpg)  

聚类算法做的事情是，把输入样本聚成围绕一些中心的『数据团』，以发现数据分布结构的一些规律。常用的聚类算法包括：

- k-Means
- Hierarchical Clustering
- Expectation Maximisation (EM)  

## 关联规则算法(Association Rule Learning Algorithms)
![git-pro-1](http://git.wangxutech.com/web/frontend/notes/books/raw/master/images/machine-learning/6.jpg)  

关联规则算法是这样一类算法：它试图抽取出，最能解释观察到的训练样本之间关联关系的规则，也就是获取一个事件和其他事件之间依赖或关联的知识，常见的关联规则算法有：

- Apriori algorithm
- Eclat algorithm

## 人工神经网络类算法(Artificial Neural Network Algorithms)
![git-pro-1](http://git.wangxutech.com/web/frontend/notes/books/raw/master/images/machine-learning/7.jpg)  

这是受人脑神经元工作方式启发而构造的一类算法。需要提到的一点是，我把“深度学习”单拎出来了，这里说的人工神经网络偏向于更传统的感知算法，主要包括：

- Perceptron
- Back-Propagation
- Radial Basis Function Network (RBFN)

## 深度学习(Deep Learning Algorithms)
![git-pro-1](http://git.wangxutech.com/web/frontend/notes/books/raw/master/images/machine-learning/8.jpg)  

深度学习是近年来非常火的机器学习领域，相对于上面列的人工神经网络算法，它通常情况下，有着更深的层次和更复杂的结构。该类算法广泛应用于计算机视觉。

- Deep Boltzmann Machine (DBM)
- Deep Belief Networks (DBN)
- Convolutional Neural Network (CNN)
- Stacked Auto-Encoders

## 降维算法(Dimensionality Reduction Algorithms)
![git-pro-1](http://git.wangxutech.com/web/frontend/notes/books/raw/master/images/machine-learning/9.jpg)  

从某种程度上说，降维算法和聚类其实有点类似，因为它也在试图发现原始训练数据的固有结构，但是降维算法在试图，用更少的信息(更低维的信息)总结和描述出原始信息的大部分内容。

有意思的是，降维算法一般在数据的可视化，或者是降低数据计算空间有很大的作用。它作为一种机器学习的算法，很多时候用它先处理数据，再灌入别的机器学习算法学习。主要的降维算法包括：

- Principal Component Analysis (PCA)
- Principal Component Regression (PCR)
- Partial Least Squares Regression (PLSR)
- Sammon Mapping
- Multidimensional Scaling (MDS)
- Linear Discriminant Analysis (LDA)
- Mixture Discriminant Analysis (MDA)
- Quadratic Discriminant Analysis (QDA)
- Flexible Discriminant Analysis (FDA)

## 模型融合算法(Ensemble Algorithms)
![git-pro-1](http://git.wangxutech.com/web/frontend/notes/books/raw/master/images/machine-learning/10.jpg)

严格意义上来说，这不算是一种机器学习算法，而更像是一种优化手段/策略，它通常是结合多个简单的弱机器学习算法，去做更可靠的决策。拿分类问题举个例，直观的理解，就是单个分类器的分类是可能出错，不可靠的，但是如果多个分类器投票，那可靠度就会高很多。常用的模型融合增强方法包括：

Random Forest
Boosting
Bootstrapped Aggregation (Bagging)
AdaBoost
Stacked Generalization (blending)
Gradient Boosting Machines (GBM)
Gradient Boosted Regression Trees (GBRT)

